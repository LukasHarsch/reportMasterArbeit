%=====================================================================
\chapter{Grundlagen}
\label{sec:Grundlagen}
%=====================================================================
TODO

\section{NN basics}
Intro:
- NN approximiert$ f^*$\\
- $y=f^*(x)$ mapping von Input x auf Klasse y\\
- $y=f(x,\theta)$ learns parameter of the NN\\
- $f^{(1)}$, $f^{(2)}$ und $f^{(3)}$ zu $f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$ gerichteter Graph\\
- $f^{(i)}$ einzelnen Ebenen des NN\\
- Input-Layer nimmt eingangsdaten auf\\
- Ausgangsebene soll Ausgabe erzeugen, möglichst ähnlich zu y\\
- Hidden layers bestehen aus vielen parallel arbeitenden einheiten, wobie jede eine vektor-zu-skalar funktion repräsentiert. (Neuron nimmt viele werte von vorherigen EInheiten als Input un berechnet eigene Aktivierung)\\

Cost funktion:\\
- wichtiger punkt ist die Kostenfunktion in der NN architektur\\
... irgendwas mit: die Kostenfunktion gibt vor welche Aufgabe das Netz lernen soll.\\
- in vielen Fällen definiert das Model eine Verteilung $p(y|x; \theta)$. So kann das Netz mit dem principle of maximum likelihood trainiert werden. Das bedeutet die Kostenfunktion ist definiert als die negative log-Wahrscheinlichkeit. Dies wird auch bezeichnet als die cross-entropy zwischen den Trainingsdaten und den model predictions. Fomal wird die Kostenfunktion beschreiben als:
\begin{equation}
J(\mathbf{\theta}) = -E\big[\mathrm{log} p_{model}(\mathbf{y}|\mathbf{x}\big].
\end{equation}
- Durch die Spezialisierung der Kostenfunktion kann das Netz so trainiert werden das es andere Schätzungen durchführt. \\
- Oft setzt sich die gesamte Kostenfunktion aus einer primären Kostenfunktuion und einem Regularizierungsterm zusammen. \\
\\
Output units:\\
- Output units eng gekopppelt mit den Output units, da die Repräsentation des Outputs die Form der Kostenfunktion beeinflusst/vorgibt.\\
- im Prinzip können diese Units überall im Netz verwendet werden. Hier liegt der Fokus aber auf ihrer Verwendung als Output des Models.\\
- Netzwerk stellt ein Set aus hidden features bereit, definiert durch $\mathbf{h})=f(\mathbf{x};\mathbf{\theta})$. Die Output units wenden eine zusätzliche Transformation auf die feature $\mathbf{h}$ an um so die Ausgabe des Netzwerkes zu vervollständign, ensprechend der definierten Aufgabe des Netzes.\\
\\
Linear:\\
- Die einfachste Form ist eine lineare Unit. Diese wendet eine affine Transformation ohne Nichtlinearität an. Eine lineare output unit produziert einen Vektort $\hat{y}= W^t h+b$ für gegebene Features $\mathbf{h}$. lineare ouputunits werden oft verwendet um Erwartungswert einer bedingten Gaussverteilung zu schätzen:\\
\begin{equation}
p(y|x)=N(y;\hat{y},I).
\end{equation}
- Maximizing the log-likelihood is then equivalent to minimizing the mean squared error. (in the linear case)\\
\\
Sigmoid:\\
- Viele Aufgaben benötigen die Schätzung einer binären Variable $y$, z.B. die Klassifikation mit zwei Klassen. \\
- Dieses Problem wird als Bernoulliverteilung  über $y$ bedingt durch $x$ modeliert. Die Bernopulliverteilung ist definiert als...(mglw hier einfügen oder weg lassen).  Somit muss das Netz nur $P(y=1|x)$ schätzen.\\ 
\\
- Um diese Bedingung zu erfüllen wird eine sigmoid unit verwendet.\\
- Dazu wird die sigmoid unit um Probleme in dieser Form zu beschreiben. \\
\\
- Die sigmopid output unit ist definiert als:
$\hat{y}=\sigma \big(w^T h + b\big)$.
Dabei ist $\sigma$ die logistic sigmoid function.\\
-----\\
- (3.10) logistic sigmoid:
\begin{equation}
	\sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}
- die logistic sigmoid funktion wird im Allgemeinen verwendet um die Parameter $\phi$ einer Bernoulliverteilung zu ermitteln, da die Funktion im Bereich (0,1) definiert ist. Somit liegt die Funktion im Wertebereich des Parameter $\phi$ der Bernoulliverteilung.\\
- die Sigmoidfunktion ist gesättigt falls ihr Argument sehr positiv oder sehr negativ ist. Somit ist die Funktion sehr flach und unempfindlich gegenüber kleinen Änderungen im Eingnang.\\
-----\\
- Zuerst lineare layer $z=w^T h + b$. Danach sigmoid aktivierung funktion um z in eine Wahrscheinlichkeit zu konvertieren.\\
\\
Softmax:\\
Die softmax Funktion kann verwendet werden um eine Wahrscheinlichkeitsverteilung über eine diskrete Variable mit $n$ möglichen Werten zu beschreiben. Im Grunde eine Verallgemeienrung der sigmoid funktion. Meistens wird die softmax Funktion als Ausgang einer Klassifikation verwendet um die Wahrscheinlichkeitsverteilung über $n$ verschiedenen Klassen zu repäsentieren.\\
- generalizierung der sigmoid funktion zur schätzung der bernoulliverteilung. 
ergibt $\hat{y}_i=P(y=i|x)$. Dabei soll nciht nur jedes Element von $\hat{y}_i$ zwischen 0 und 1 liegen sonder auch die Summe des gesamten Vektors = 1 sein.\\
- Somit gilt der Ansatz allgemein für multinoulli verteilungen\\
- Wie zuvor wird zuerst eine lineare Layer $z=w^T h + b$ angewendet. Anschließend wird die softmax aktivierung angewendet, beschreiben durch: $\mathrm{softmax}(z)_i = \frac{e^{z_i}}{\sum_j e^{z_i}}$\\  
\\
- vllt beispiel aus Präsi wie beliebige Werte über die softmax in Verteilung umgerechnt werden + Bild von MNIST mit zugehörigem Blakendiagram für die Verteilung\\
\\
Hidden layer:\\
- Im allgemeinen werden die hidden units als Funktion beschreiben welche auf einen Vektor aus Eingangsdaten \textbf{x} eine affine Transformation $z=W^Tx+b$ durchführt. Dabei beschreibt $W$ eine Gewichtsmatrix und $b$ einen Bias-Vektor. Anschließend wird eine elementweise nichtlineare Funktion $g(z)$ angewendet. Die Funktion $g(z)$ wird Aktivierungsfunktion genannt.\\
- hidden unit für jede Ebene werden somit durch $h^{(l)}=g(W^{(l)T}x+b^{(l)})$ beschrieben, wobei $l$ die jeweile Ebene vorgibt.\\
- Die meisten hidden units unterscheiden sich durch eine unterschiedliche Wahl der Aktivierungsfunktion $g(z)$.\\
\\
- häufig werden rectified linear units (ReLU) verwendet mit der aktivierungsfunktion:
$g(z)=max\{0,z\}$.\\
- Diese verhalten sich wie eine lineare Aktivierung nur dass die Outputs über die linke Hälfte der Domäne =0 sind.\\
- Relun wird typischerweise ontop einer affinen Transformation angewandt:
$h=g(W^Tx+b)$.\\
\\
- Erweiterung ist die leaky ReLU $f(x)= 
\begin{cases}
	x & if x > 0\\
	\alpha x & otherwise
\end{cases}
$\\
- Darüberhinaus existieren viele weitere Aktivierungsfunktionen.\\
\\
Conv netzworks:\\
- Erweiterung der bisher beschreibenen NN sind die convolutional neural networks (CNN).\\
- Großen Erfolg in der Bildverarbeitung, bei welchem der Input als 2D grid von pixeln betrachtet werden kann.\\
- wie der Name sagt wird in diesen Netzen eine convolituional Operator eingeführt.\\
\\
Conv operator:\\
- Convolution im Allgemeinen ist eine Operation zwischen zwei Funktionen $x$ und $w$. Dabei wird das Integral der punktweisen Multiplikation der zwei Funktionen gebildet, wobei eine Funktion gedreht und verschoben ist. - Dies ist definiert als:
\begin{equation}
s(t)=(x\star w)(t)=\sum_{a=-\infty}^{\infty}x(a)w(t-a).
\end{equation}
- Bei der Anwendung von CNNs ist der Eingang typischerweise ein multidimensionaler Array aus Daten. Der Kernel beschreibt ein mutlidimensionalen Array aus Parameter die während des Trainings vom Lernalgorithmus angepaqsst werden.\\
- oftmals wird convolution über mehr als eine Achse berechnet. Für ein zweidimensionals Bild $I$ als Input beispielsweise kann ein zweidimensionaler Kernel $K$ verwendet werden. Die Convolution resultiert in:
\begin{equation}
S(i,j)=(I\star K)(i,j)=\sum_{m}\sum_{n} I(m,n)K(i-m,j-n).
\end{equation}
- Erwähnen, dass für die Implementierung einige Umformungen angewendet werden, flipping; cross-correlation 
- Bild von Faltungsprozess und Referenze einfügen.\\
%
\textbf{Motivation}\\
- CNNs haben drei Ideen welche heflen das machine learning Verhalten zu verbessern: sparse interactions, parameter sharing and equivariant
representations.\\
- Traditionelle NN Ebenen verwenden eine Matrix multiplikation zwischen einer Matrix aus Parametern und einem weiteren Paramater. Das bedeutet jede Output unit interagiert mit jeder Eingangs unit. CNN dagegen haben eine \textbf{sparse interaction}. Dies wird erreicht indem kleiner als der Input ist. Ein Bild beispielsweise besteht aus einer vielzahl von Pixeln. Ein kleinerer Kernel betrachtet nur wenige Pixel aus dem gemsaten Bild und kann somit aussagekräftige Features wie Kanten und Ecken in dem Bild erkennen. Darüber hinaus werden weniger Parameter benötigt wodurch der Speicher- und Rechenaufwand reduziert wird.\\
- \textbf{parameter sharing:} In traditionlellen NN wird jedes Element der Gewichtsmatrix genau ein mal verwendet um den Ausgang einer Ebene zu berechnen, indem jedes Gewicht mit einem Element des Inputs multipliziert wird und danach nicht mehr verwendet wird. In einem CNN dagegen wird jeder Filter eines Kernels an jeder Position des Inputs verwendet. Somit wird an Stelle eines seperaten Parameterset für jede Position im Input nur ein Parameterset für alle Positionen benötigt.  \\ 
%
- Des Weiteren haben CNN die Eigenschaft \textbf{equivariance to translation}. Für eine Funktion bedeutet dies, wenn sich der Input ändert, so ändert sich der Output gleichermaßen. Formel geschrieben ist eine Funktion $f(x)$ equivariant zu einer Funktion $g$ falls $f(g(x))=g(f(x))$ ist.\\
- Beispielsweise soll die Funktion $I$ die Pixel eines Bildes beschreiben. Das Mapping auf eine andere Funktion soll beschreiben werden durch $I'=g(I)$, wobei gilt $I'(x,y)=I(x-1,y)$. Dadurch wird jedes Pixel in dem Orginalbild um eine Einheit verschoben. Durch die equivariance Eingenschaft ergibt sich der selbe Output wenn zuerst die Transformation $g$ und dann die Convolution angewendet wird bzw. erst die Conv, dann die Trafo $g$.\\
- Diese Eigenschaft ist beispielsweise in der Bildverarbeitung sehr hilfreich. Hier kann die erste Conv-Ebene Feautres wie z.B. Ecken oder Kanten erkennen. Da Ecken und Kanten über das gesamte Bild verteilt auftreten können ist es hilfreich die Paramter über das gesamte Bild zu teilen. \\
%
\\
\\
%
\textbf{Pooling}
%
-Der Conv-Oprator tritt oftmals in Verbindung mit dem Pooling-Operator auf. Das Pooling wird auf den Output der Conv-Ebene angewendet und ersetzt diesen durch eine Zusammnfassung der nahegelegenen Outputs. Der max Pooling-Operator beispielsweise fasst eine rechteckige Umgebung der Outputs zusammen durch Ausgabe des maximalen Werts in dieser Umgebung. \\
- Darüberhinaus existiren viele weitere Pooling-Verfahren.\\
- Ziel des Pooling ist es die Filter invariant gegnüber kleinen Veränderungen im Input zu machen. Invariant bedeutet, dass durch eine kleine Änderung des Inputs die meisten pooled Outputs unverändert bleiben.\\
- Diese Eingenschaft ist nützlich, da es oftmals von größerem Interesse ist zu erkennen ob Features existieren anstatt deren exakte Position zu erhalten.\\
- Beispielsweise in der Gesichtserkennung ist es ausreichen zu erkennen, dass sich ein Auge auf der rechten Seite und eines auf der linken Seite befindet, anstatt die Position der Augen auf das pixel genau zu bestimmen. 
- Auch ist es möglich die Pooling-Region um jeweils $k$ Pixel zu verschieben. Dies führt dazu, dass die Anzahl der Output units ca. $k$-mal weniger werden und somit deutlich weniger Inputs in der nächsten Ebene verarbeitet werden müssen. Dadurch kann der Rechenaufwand des Netzes verringert werden.\\
%
\textbf{Optimization}\\
%
- Ein neuronales Netz ist definert als eine Funktion $y=f(x,\theta)$, welche Abhängig von den Parametern $\theta$ einen Input $x$ auf den Ausgang $y$ mappt. Dabei gilt es $\theta$ so anzupassen, dass $f$ möglichst exakt die gewünschte Funktion $f^*$ approximiert. Diese Anpassung geschieht während des Trainingsprozesses. Das Training ist ein Optimierungsproblem mit dem Ziel die Parameter $\theta$ zu finden, welche die Kostenfunktion $J(\theta)$ minimiert.\\
- P performance meassurement...nicht direkt verbunden mit $J$...\\
- Die Kostenfunktion wird im allgemeinen als Mittelwert über das gesamte Trainingsset beschreiben:
\begin{equation}
J(\theta)=\mathbb{E}_{(x,y)\sim\hat{p}_{data}} L(f(x;\theta),y).
\end{equation}
Dabei ist $L$ die Kostenfunktion für ein einzelnes Element, $f(x;\theta)$ ist die geschätzte Ausgabe des Netzes für den Eingang $x$, $y$ die Zielausgabe und $hat{p}_{data}$ beschreibt die Verteilung des Trainingdatensets.\\
- vllt suprvized und unregularizd anmrken\\
- empirische Verteitung zum training verwenden, in der hoffnung dass sich das Loss für die allgemeine Verteilung verringert (empirical risk minimization to to hopefully minimize the overall risk)\\
- Das Update der Paramter in einer Optimierung von machine learning geschieht normalerweise durch Ermittlung der Kostenfunktion aus einem Subset des gesamten Trainingsset. Beispielsweise die maximum log-Likelihood Schätzung wird beschreiben als Summe über den gesamten Datensatz:\\
...equation...\\
...umformung in kostenfunktion...\\
...wichtigste Eignschaft der Gradient...\\
...sehr teuer für gsamten Datensatz -> nur auf kleinen Teilset berechnen...\\
...weiterer Grund redundanz im Datensatz -> Viele Daten sehr ähnlich und somit ähnliches contribution on gradient\\
...batch, minibatch, SGD eigentlich 1 bild...liegt in wirklichkeit dazwischen\\
%
%
\\goodfellow S.274\\
%
%
%
%
%
\section{Neuronale Netze}
%\begin{comment}
- NN approximiert$ f^*$\\
- $y=f^*(x)$ mapping von Input x auf Klasse y\\
- $y=f(x,\theta)$ learns parameter of the NN\\
- $f^{(1)}$, $f^{(2)}$ und $f^{(3)}$ zu $f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$ gerichteter Graph\\
- $f^{(i)}$ einzelnen Ebenen des NN\\
- Input-Layer nimmt eingangsdaten auf\\
- Ausgangsebene soll Ausgabe erzeugen, möglichst ähnlich zu y\\
- Hidden layers bestehen aus vielen parallel arbeitenden einheiten, wobie jede eine vektor-zu-skalar funktion repräsentiert. (Neuron nimmt viele werte von vorherigen EInheiten als Input un berechnet eigene Aktivierung)\\
- Im allgemeinen werden die hidden units als Funktion beschreiben welche auf einen Vektor aus Eingangsdaten \textbf{x} eine affine Transformation $z=W^Tx+b$ durchführt. Dabei beschreibt $W$ eine Gewichtsmatrix und $b$ einen Bias-Vektor. Anschließend wird eine elementweise nichtlineare Transformation $g(z)$ angewendet. Die Transformation $g(z)$ wird aktivierungsfunktion genannt. \\
- hidden unit für jede Ebene werden somit durch $h^{(l)}=g(W^{(l)T}x+b^{(l)})$ beschrieben, wobei $l$ die jeweile Ebene vorgibt.\\

- häufig werden rectified linear units verwendet mit der aktivierungsfunktion $g(z)=max\{0,z\}$.\\
- Erweiterung ist die leaky ReLU $f(x)= 
\begin{cases}
x & if x > 0\\
\alpha x & otherwise
\end{cases}
$\\
- Es existieren viele weitere Aktivierungsfunktionen.\\



%\end{comment}
%
%
%
%
%
In diesem Kapitel werden wichtigsten Grundlagen von neuronalen Netzen (NN) erläutert. Das Ziel von neuronalen Netzen ist es eine Funktion $f^*$ zu approximieren. Für eine Klassifikation beispielsweise mappt die Funktion $y=f^*(x)$ die Eingangsdaten $x$ auf eine Klasse $y$. Das Mapping in einem neuronalen Netz ist definiert als $y=f(x;\theta)$, wobei das Netz die Werte der Parameter $\theta$ lernt um die Funktion bestmöglichst zu approximieren. Die Zusammensetzung der Funktionen in einem NN können als Model eines gerichteten Graphen betrachtet werden. Ein Beispiel ist die Verkettung von $f^{(1)}$, $f^{(2)}$ und $f^{(3)}$ zu $f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$. Die Funktionen $f^{(i)}$ beschreiben dabei die verschiedenen Ebenen in einem NN. Während des Trainings wird versucht $f(x)$ so zu berechnen das sie zu $f^*(x)$ passt. Das Trainingssample aus $(x,y)$ gibt dabei vor, dass die Ausgabeebene des Netzes einen Wert produzieren soll, welcher möglichst ähnlich zu $y$ ist. Das Verhalten der inneren Ebenen wird nicht direkt vorgegeben. Der Trainingsalgorithmus muss dann entscheiden wie die inneren Ebenen zu verwenden sind um $f^*$ zu approximieren und die gewünschte Ausgabe zu erhalten.\\   
%
%
%
%
%
%
Hierzu wird beispielhaft ein simples feedforward NN betrachtet. Der Aufbau wird in Abbildung XYZ veranschaulicht. Ein CNN besteht aus einer \textbf{Eingangsebene}, mehreren \textbf{versteckten Ebenen} im inneren und einer \textbf{Ausgangsebene}. 
%
\begin{itemize}
 \item input
 \item hidden layer
 \item output softmax
 \item convolution discrete equaltion, maybe image, kernel is learning filter, imgage of real-world filter 
 \item activation sigmoid, relu
 \item striding
 \item flat
 \item fully connected layer
 \item Training (SGD)
\end{itemize}
%
%
\section{Semi-überwachtes Lernen}
Das Themengebiet maschinelles Lernen umfasst zwei grundsätzlich verschiedene Aufgabentypen, \textbf{unübrwachtes} und \textbf{übrwachtes} Lernen. Für unüberwachtes Lernen wird ein Set $X=(x_1,\ldots,x_n)$ aus $n$ Exmplaren/Datenpunkten/Samples benötigt, wobei $x_i \in \mathcal{X}$ für alle $i \in [n] := {1,\ldots,n}$. Es wird davon ausgegangen, dass die Punkte unabhängig und gleichverteilt (i.i.d.) von einer Verteilung $\mathcal{X}$ mit der Dichte $p(x)$ gezogen werden. Jeder Eingang $x_i$ stellt ein seperates Feature darstellt. In einem Bild beispielsweise sind die Features die jeweiligen Werte der Pixel. Das Zeil von unüberwachtem Lernen ist es interessante Strukturen in den Daten $X$ zu finden. Dabei ist es üblich die Dichte oder ein bekanntes Funktional zu schätzen welches mit hoher Wahrscheinlichkeit $X$ generiert. Viele Techniken für die Dichteschätzung benötigen eine Latentvariable (unüberwachtes Klassenlabel) $y$. Die Latentvariable $y$ ist eine für das Problem entsprechend modellierte Größe und nicht zu vergleichen mit Klassenlabel bei einer Klassifikation, welches real Klasse wiederspiegeln soll. $y$ wird anschließend verwendet um $P(x)$ als eine gemischte Verteilung $\sum_{y=1}^M P(x|y)P(y)$ zu schätzen. Weitere Anwendungen für unüberwachtes Lernen sind biespielsweise Clustering, Outliererkennung oder Dimensionsreduzierung.\\
%
Im Gegensatz zu unüberwachtem Lernen benötigt überwachtes Lernen ein Trainingsdatenset aus Paaren von $(x_i,y_i)$. Dabei beschreibt $y_i$ das Klassenlabel oder das Ziel von $x_i$. Das Ziel von überwachtem Lernen ist die Schätzung eines funktionalen Zusammenhangs bzw. ein mapping von $x\rightarrow y$. Dabei gilt es die Wahrscheinlichkeit des Klassifikationsfehlers zu minimieren. Bei einer Klassifikation kann so durch Schätzung der Dichte $p(y|x)$ den Eingangsdaten $X$ die zugehörigen Klassenlabels $y$ zugewiesen werden. In dieser Arbeit wird das überwachte Lernen verwendet um \textbf{Convolutional Neural Networks} zu trainieren. Ziel des Netzes ist es eine Funktion $f*$ zu approximieren. Für eine Klassifikation mappt die Funktion $y=f*(x)$ die Eingangsdaten $x$ auf eine Klasse $y$. Das Mapping in einem neuronalen Netz ist definiert als $y=f(x;\theta)$ wobei das Netz die Werte der Parameter $\theta$ lernt um die Funktion bestmöglichst zu approximieren.\\
%
Ein weiterer Aufgabentyp ist das \textbf{semi-überwachte} Lernen (SSL), welcher zur kathegorie des des überwachten Lernens gehört, da auch hier das Ziel ist den Klassifikationsfehler zu minimieren. Wie zuvor wird ein Datenset mit den zugehörigen Labeln $D_l=\{(x_i,y_i)|i=1,\ldots,n\}$ benötigt, welche i.i.d. von $P(x,y)$ gezogn wurden. Darüber hinaus existiert ein Datenset ohne Klassenlabel $D_u=\{x_{n+j}|j=1,\ldots,m\}$ aus der Verteilung$P(x)$. Semi-überwachtes Lernen ist besonders interessant wenn $m>>n$, da ungelabelete Daten oftmals günstiger und einfacher zu erhalten sind als gelabelte Daten. Das vorgehen bei SSL bsteht aus zwei Schritten. Im ersten werden Methoden des unüberwachten Lernens verwendet um mit Hilfe einer Latentvariabel $y$ die Verteilung $P(x)$ zu schätzen. Anschließend können die Latentgruppen mit den beobachteten Klassen aus $D_l$ verbunden werden. Dieses Vorgehen wird bei den Verfahren \textbf{Variational Autoencoder} und \textbf{Generative Adversarial Network} verwendet.\\  
%http://www.acad.bg/ebook/ml/MITPress-%20SemiSupervised%20Learning.pdf
\cite{Goodfellow-et-al-2016}
%
%
%
\subsection{Generative Netze}
Das Ziel von maschinellem Lernen ist die Approximation eines Modells das möglichst allgemein gilt und somit neue bisher unbekannte Daten verarbeiten kann. Der beste Weg solch ein Model zu verallgemeinern ist es, dass Model mit mehr Daten zu trainieren. Allerdings ist die Menge der Trainingsdaten oft limitiert. Aus diesem Grund können Fake-Daten generiert werden um den Datensatz künstlich zu erweitern. Bei einer Klassifikation beispielsweise soll den Eingangsdaten $\mathbf{x}$ eine ein Label $y$ zugewiesen werden. Das bedeutet der Klassifikator muss möglichst invariant gegenüber einer breiten Reihe von Transformationen sein. Neue Trainingsdaten $(\mathbf{x},y)$ können somit einfach durch eine Transformation der Eingangsdaten $\mathbf{x}$ aus dem ursprünglichen Trainingsset erzeugt werden. Bekannte Methoden sind z.B. Umwandlung einiger Pixel in einem Bild, Rotation des Bildes oder eine Veränderung der Skalierung des Bildes. Auch kann es bereits genügen den Eingangsdaten ein Rauschen hinzuzufügen um den Datensatz zu erweitern.\\
\cite{Goodfellow-et-al-2016}
\\
%
Darüber hinaus existieren weitere komplexere Methoden um Fake-Daten künstlich zu erzeugen. Diese werden generative Modelle genannt. Ziel ist es neue, ungesehene Daten zu erzeugen die ähnlich zu den ursprünglichen Daten sind aber nicht genau gleich sind. Dazu wird ein Set aus Daten $X$ mit einer unbekannten Verteilung $P_{unb.}(X)$ benötigt. Es gilt ein Model zu trainieren welches eine Verteilung $P(X)$ lernen soll, sodass $P(X)$ möglichst ähnlich zu $P_{unb.}(X)$ ist. Aus dieser gelernten Verteilung $P(X)$ kann gesamplet werden um neue Daten zu erzeugen. In dieser Arbeit werden zwei Arten von generativen Netzen untersucht, der "variational autoencoder" und das "generative adversarial network".\\   
\cite{2016arXiv160605908D}
%
%
Generative adversarial networks are an example of generative models. The term ?generative model? is used in many different ways. In this tutorial, the term refers to any model that takes a training set, consisting of samples drawn from a distribution $p_{data}$, and learns to represent an estimate of that distribution somehow. The result is a probability distribution $p_{model}$. In some cases, the model estimates $p_{model}$ explicitly, as shown in figure 1. In other cases, the model is only able to generate samples from $p_{model}$, as shown in figure 2. Some models are able to do both. GANs focus primarily on sample generation, though it is possible to design GANs that can do both.
%
%
\subsubsection{Variational Autoencoder}
%
%
\begin{figure}
%
\tikzstyle{int}=[draw]
\begin{tikzpicture}
	[auto,>=latex']
	\node [int, regular polygon,regular polygon sides=3, shape border rotate=-90] (Enc) {\begin{tabular}{c} Encoder \\ $Q(z|X)$ \end{tabular}};
	\node (Input) [left of=Enc, node distance=4cm, coordinate] {Input};
	\node [int, rectangle, minimum width=0.5cm, minimum height=2cm] (Lat) [right of=Enc, node distance=4cm] {$z$};
	\node (Middle2) [left of=Lat, node distance=3cm, coordinate] {Middle2};
	\node [int, regular polygon,regular polygon sides=3, shape border rotate=90, label=above:{Decoder}] (Dec) [right of=Lat, node distance=4cm] {\begin{tabular}{c} Decoder \\ $P(X|z)$ \end{tabular}};
	\node (Middle) [left of=Dec, node distance=3cm, coordinate] {Middle};
	\node [coordinate] (end) [right of=Dec, node distance=4cm]{};
	
	\path[->] (Input) edge node {Input $X$} (Enc);
	\path[->] (Enc) edge node {} (Lat);
	\path[->] (Lat) edge node {} (Dec);
	\draw[->] (Dec) edge node {Output $\hat{X}$} (end) ;
\end{tikzpicture}
%
\caption[VAEblock]{Blockschaltbild eines Autoencoders}
\label{VAEblock}
\end{figure}
%
%
Die Grundidee eines Autoencoders (AE) ist die Eingangsdaten über einen Zwischenraum auf die Ausgangsdaten zu kopieren. Dabei beschreibt der Zwischenraum einen Code welcher die Eingangsdaten repräsentiert. Der AE besteht aus zwei hintereienander gschalteten Netzen. Der strukturelle Aubau wird in Abbildung \ref{VAEblock} dargestellt. Das erste Netz, der \textbf{Encoder} $Q$ ist eine Funktion $z=f(X)$ welche die Eingangsdaten $X$ in einen Zwischenraum $z$ mappt. Der Zwischenraum wird im folgenden als Latentraum bezeichnet. Das zweite Netz, der \textbf{Decoder} $P$ soll aus den kodierten Daten $z$ die ursprünglichen Daten $X$ rekonstruieren. Der Autoencoder soll dabei nicht nur das Mapping $g(f(x))=x$ lernen. Oft wird der AE so eingeschränkt das er wichtige Eigenschaften der Eingangsdaten lernt. So kann der AE beispilsweise die Verteilung $P(X)$ der Eingangsdaten lernen.\\
%
Der \textbf{Latentraum} ist eine Vektor $\mathbf{z}$ aus sogenannten Latentvariablen $z$. Dabei können Samples von $\mathbf{z}$ entsprechend einer Wahrscheinlichkeitsdichtefunktion (PDF) $P(\mathbf{z})$ entnommen werden. Im weiteren Verlauf wird $\mathbf{z}$ als normal verteilt angenommen, d.h. $\mathbf{z}\sim\mathcal{N}(0,I)$, wobei $I$ die Einheitsmatrix darstellt. Die multidimensionale Verteilung von $P(\mathbf{z})$ in $d$ Dimensionen setzt sich somit aus einem Set von $d$ normal verteilten Variablen $z$ zusammen.\\
%
Wie breits erwähnt ist das Ziel ein Model der Daten zu finden, d.h. der AE soll die Verteilung $P(X)$ der Eingangsdaten $X$ lernen. Dazu wird eine Funktion $X=f(z;\theta)$ benötigt, welche die Latentvariablen $z$ nach $X$ mappt. Der Vektor $\theta$ definiert die Parameter der Netzes. Es gilt $\theta$ so zu optimieren, dass durch ziehen von Samples $z$ aus $P(z)$ die Funktion $f(z;\theta)$ mit hoher Wahrscheinlichkeit Ausgaben produziert welche gleich den $X$'s aus dem Datenset sind. Dieser Zusammenhang kann nach dem Gesetz der totalen Wahrscheinlichkeit beschreiben werden als:
%
\begin{equation}
P(X)=\int \underbrace{P(X/z,\theta)}_{f(z;\theta)}P(z)dz.
%\label{eq:totaleWahrscheinlichkeit}
\end{equation}
%
Wenn das Model mit hoher Wahrscheinlichkeit Beispiele aus dem Trainingsdatensatz produziert ist es ebenfalls sehr wahrscheinlich, dass das Model Samples erzeugen kann die ähnlich aber nicht gleich den Trainingsdaten sind.\\
%
Das bisherige Konzept wird im Folgenden zu einem variational Autoncoder (VAE) erweitert. Hierbei ist das Ziel die Verteilung $P(z)$ aus $P(z|X)$ zu erschließen. Dies macht Sinn, da $z$ so eine Verteilung annimmt welche mit hoher Wahrscheinlichkeit $X$ produzieren kann. Diese Methode wird variational inference (VI) genannt. Die Idee ist die tatsächliche Verteilung $P(z|X)$ mittels einer einfacheren Verteilung $Q(z|X)$ zu modellieren, z.B. eine Gaussverteilung. Dabei gilt es den Unterschied zwischen der tatsächlichen und der modellierten Verteilung zu minimieren. Dazu wird die Kullback-Leibler (KL) Divergenz verwendet. Diese Metric beschreibt den Unterschied zweier Verteilungen, hier den Unterschied zwischen $P(z|X)$ und $Q(z|X)$. Die KL Divergenz wird formliert als:
%
\begin{equation}
D_{KL}\big[Q(z|X)||P(z|X)\big] = E\big[\mathrm{log} Q(z|X) - \mathrm{log} P(z|X)\big].
\end{equation}
%
Mit der Anwendung der Bayes-Regel und einigen Transformationen ergibt sich daraus die Kostenfunktion für den VAE als:
%
\begin{equation}
\mathrm{log} P(X) - D_{KL}\big[Q(z|X)||P(z|X)\big] = E\big[ \mathrm{log} P(X|z)\big] - D_{KL}\big[Q(z|X)||P(z)\big].
\label{eq:KL_loss}
\end{equation}
%
Die linke Seite der Gleichung \ref{eq:KL_loss} beschreibt die kostenfunktion des VAE. Wie bereits erwähnt ist das oberste Ziel des VAE die Wahrscheinlichkeit von $P(X)$ zu maximieren. Hier beschreiben durch die log-Wahrscheinlichkeit $\mathrm{log} P(X)$. Hinzu kommt der Regularizierungsterm $D_{KL}\big[Q(z|X)||P(z|X)\big]$ welcher sicherstellt, dass $Q$ Latentvariablen $z$ produzieren kann aus denen sich anschließend $X$ reproduzieren lässt. Die rechte Seite von Gleichung \ref{eq:KL_loss} ist eine Darstellung die mittels stochastic gradient descent Methoden optimiert werden kann. Wie im oberen Teil bereits bschreiben wird die Verteilung von $P(z)$ als Standartnormalverteilung $\mathcal{N}(0,I)$ definiert. Somit soll auch $Q(z|X)$ eine Gaussverteilung mit Erwartungswert $\mu(X)$ und Varianz $\Sigma(X)$ werden. Für die KL Divergenz ergibt sich durch Einsetzen diesen beiden Verteilungen:
%
%
\begin{equation}
 D_{KL}\big[N(\mu(X),\Sigma(X))||N(0,I)\big] = \frac{1}{2}\sum_k\Big(\Sigma(X)+\mu^2(X)-1-\mathrm{log}\Sigma(X)\Big).
\end{equation}
%
- Um den Eingang $z$ für den Encoder zu erhalten müssen Sample aus eine Gaussverteilung gezogen werden, welche durch die Parameter aus dem DEcoder definiert wird. Dies führt allerdings zu Problemen bei der Optimierung mit gradient descent, da der Sampling-Prozess keine Gradienten besitzt.\\
- Damit das Netz differenzierbar bleib wird der sogenannte Reparametrisierungstrick angewandt.
- Eine Standartnormalverteilung lässt sich in jede Gaussverteilung konvertieren, falls deren Erwartungswert und Varianz bekannt sind. Somit genügt es Sample aus einer Standartnormalverteilung $\epsilon\sim N(0,1) $ zu ziehen und diese mit dem Erwartungswert $\mu$ und der Varianz $\Sigma$ aus dem Encoder nach $z$ zu konvertieren:
\begin{equation}
z=\mu(X)+\Sigma^{\frac{1}{2}}(X)\epsilon.
\end{equation}
Somit wird der eigentliche Sampling-Prozess aus dem Netz herausgezogen. Lediglich $\mu$ und $\Sigma$ sind noch Teil des Netzes und die Backpropagation kann problemlos durchgeführt werden. 
%
%
\subsubsection{ACGAN}
Ein weiteres generatives Model ist das sogenannte \textbf{generative adversarial network} (GAN). Wie im Model des VAE zuvor ist auch hier das Ziel künstliche Daten aus einem zufällig verteilten Rauschvektor $z$ zu generieren. Das GAN besteht aus zwei Netzen, dem \textbf{Generator} $\mathbf{(G)}$ und einem \textbf{Diskriminator} $\mathbf{(D)}$ die gegeneinander trainiert werden. Der Generator nimmt als Input einen zufälligen Rauschvektor $z$ und soll daraus fingierte Bilder $X_{fake}=G(z)$ erzeugen, welche sich möglichst nicht mehr von den realen Bildern unterscheiden lassen. Die Aufgabe des Diskriminators ist es die Bilder in die Kategorien $fake$ und $real$ zu klassifizieren. Dazu nimmt $D$ entweder ein reales oder fingiertes Bild als Input. Daraus soll $D$ eine Wahrscheinlichkeitsverteilung $P(S|X)=D(X)$ über die möglichen Quellen der Bilder bestimmen, also $fake$ oder $real$.\\
%
Der Diskriminator soll so trainiert werden, dass er die log-Wahrscheinlich maximiert für welche der Input $X$ der richtigen Quelle zugeordnet wird. Daraus ergibt sich die Kostenfunktion für den Diskriminator:
\begin{equation} \label{LossGAN}
L=E\big[\mathrm{log} P(S=real |X_{real})\big]+ E\big[\mathrm{log} P(S=fake|X_{fake})\big]
\end{equation}
Um den Generator zu trainieren wird dagegen versucht die log-Wahrscheinlich mit welcher $X_{fake}$ der Klasse $fake$ zugeordnet wird zu minimieren. D.h. es wird versucht den zweiten Term aus Gleichung \ref{LossGAN} zu minimieren. Das bedeutet der Generator versucht Bilder zu erzeugen, welche mit hoher Wahrscheinlichkeit der Klasse $real$ angehören. 
%
%
\\https://arxiv.org/pdf/1610.09585.pdf\\
\\https://arxiv.org/pdf/1701.00160.pdf\\
%
%
\subsubsection{ACGAN2}
%
Ein weiteres generatives Model ist das sogenannte \textbf{generative adversarial network} (GAN). Das GAN besteht aus zwei Netzen, dem \textbf{Generator} $\mathbf{(G)}$ und einem \textbf{Diskriminator} $\mathbf{(D)}$ die gegeneinander trainiert werden.
- Ziel des Generators ist es Samples zu generieren die mit hoher Wahrscheinlichkeit aus der selben Verteilung stammen wie die Trainingsdaten.\\
- Der Diskriminator  soll erkennen ob es sich um fingierte Samples aus dem Generator handelt oder um reale Samples aus dem Trainingsdatenset. Dazu klassifiziert $D$ die Daten in zwei Klassen $fake$ oder $real$.\\
- Die beiden Netze werden gegeneinander trainiert, sodass der Generator lernt den Diskriminator auszutricksen indem $G$ lernt, Samples zu generieren welche aus der gleichen Verteilung gezogen wurden wie die Trainingsdaten. 
- GANs are a structured probabilistic model containing latent variables z and observed variables x\\
- \textbf{diesen teil vllt in Unterkapitel Generator übernehmen}
- Das Diskriminatornetz kann als Funktion $D$ mit den Netzwerkparametern $\theta^{(D)}$ beschreiben werden, welches Inputdaten $x$ aufnimmt.\\
- Dementsprechend wird das Generatornetz als Funktion $G$ mit den Parametern $\theta^{(G)}$ beschreiben. $G$ erhält als Input einen zufälligen Rauschvektor $Z$.\\
- Beide Netze haben Kostenfunktionen die jeweils abhängig von den Parametern beider Netze sind. Der Diskriminator versucht die Kostenfunktion $J^{(D)}(\theta^{(D)}, \theta^{(G)})$ zu minimieren, wobei er nur $\theta^{(D)}$ verändern kann. Genauso versucht der Generator $J^{(G)}(\theta^{(D)}, \theta^{(G)})$ zu minimieren indem $\theta^{(G)}$ angepasst wird.\\
\\
\textbf{fig. 12 aus Paper infügen und erklären}\\
\\
\textbf{Generator:}\\
- Generator $G$ ist differenzierbare Funktion. In diesem Fall ein neuronales Netz.
- $z$ ist ein zufälliger Rauschvektor, dessen Sample aus einer beliebigen einfachen prior Distribution gezogen werden. Oftmals genügt hierfür eine Gleichverteilung $z\sim\mathcal{U}(a,b)$ oder eine Normalverteilung $z\sim\mathcal{N}(0,1)$. \\
- Für einen Rauschvektor $z$ soll der Generator $G(z)$ ein Sample $x$ generieren, welches aus der Verteilung $p_{model}$ gezogen wurde.\\
\\
\textbf{Training:}\\
Für das Training der beiden Netze werden zwei SGD Optimierungen gleichzeitig verwendet. In jeder Iteration werden zwei Batches geladen. Ein Batch mit Samples $x$ aus den Trainingsdaten und ein Batch mit Samples $z$ gezogen aus einer prior Distribution. Anschließend werden zwei Gradientenschritte gleichzeitig durchgeführt. In einem Schritt wird $\theta^{(D)}$ angepasst um $J^{(D)}$ zu minimieren, während im anderen das gleiche für $\theta^{(G)}$ und $J^{(G)}$ gilt.\\
\\
\textbf{Kostenfunktion:}\\
- Standardmäßig wird  für die Klassifikation von binären Problemen mit Sigmoid Outputs die Cross-Entropie Kostenfunkiton verwendet.\\
- Diese wird im Grunde auch als Kostenfunkiton für den Diskriminator angewandt:
\begin{equation}
J^{(D)}(\theta^{(D)}, \theta^{(G)}) = -\frac{1}{2}E_{x\sim p_{data}}\mathrm{log}D(x) -\frac{1}{2}E_{z}\mathrm{log}(1-D(G(x)))
\end{equation}
In der GAN Architektur bekommt der Klassifizierer allerdings zwei Batches für das Training. In einem Batch sind die Samples aus dem Trainingsdatenset, welche alle das Label 1  erhalten, während die Samples aus dem Generator im zweiten Batch mit 0 gelabelt werden.\\
\\
\textbf{Gesamtkonzept:}\\
- Das GAN Konzept besteht aus zwei Szenarien.\\
- Im ersten Szenario wird ein Batch aus zufälligen Samples $x$ aus dem Trainingsdatenset als Input für den Diskriminator verwendet. Ziel des Diskriminators ist es eine Wahrscheinlichkeit auszugeben, ob der Input real oder fake ist. In diesem ersten Schritt soll $D(x)$ eine Wahrscheinlichkeit nahe 1 ausgeben, also alle Samples aus $x$ als real klassifizieren. \\
- Im zweiten Schritt wird $z$ als Input für den Generator $G$ verwendet. Der Diskriminator wiederum erhält $G(z)$ als Input, also fake Samples aus dem Generator. Der Diskriminator versucht für $D(G(z))$ eine Wahrscheinlichkeit von 0 zu erhalten und somit die Bilder als fake zu erkennen. Währenddessen bemüht sich der Generator  für $D(G(z))$ eine Wahrscheinlichkeit von 1 zu verursachen.\\
- Wird dieser Prozess erfolgreich durchgeführt bedeutet dies, dass $G(z)$ aus der selben Verteilung wie die Trainingsdaten gezogen werden.\\
\\
\textbf{ACGAN:}\\
Bislang hat das GAN lediglich die Möglichkeit neue ungesehene Daten aus einer prior distributuion zu Generieren, sowie reale Daten von Fake Daten zu unterscheiden.\\
- Diese GAN Architektur lässt sich zu einem \textbf{auxiliary classifier GAN} (ACGAN) erweitern.\\
- In einem ACGAN besitzt jedes generierte Sample ein zugehöriges Klassenlabel. Dazu nimmt der Generator $G$ zusätzlich zu einem Rauschvektor $z$ das Klassenlabel $c\sim p_c$ als Input. Der Generator nutzt somit beides, $z$ und $c$ um fingierte Bilder $X_{fake}=G(c,z)$ zu erzeugen.\\
- Auch der Diskrimator kann beide Informationen verarbeiten, indem er eine Wahrscheinlichkeitsverteilung über die Quelle der Samples (real/fake) und eine Wahrscheinlichkeitsverteilung über die Klassenlabel ausgibt:
\begin{equation}
P(S | X), P(C | X) = D(X).
\end{equation}
- Ebenso wird die Kostenfunktion um einen zweiten Teil erweitert. Der erste Term bleibt wie im standard GAN die log-Wahrscheinlichkeit für die korrekte Quelle $L_S$. Für das ACGAN wird die log-Wahrscheinlichkeit für die korrekte Klasse $L_C$ hinzugefügt. 
\begin{equation} \label{LossACGAN_S}
L_S=E\big[\mathrm{log} P(S=real |X_{real})\big]+ E\big[\mathrm{log} P(S=fake|X_{fake})\big]
\end{equation}
%
\begin{equation} \label{LossACGAN_C}
L_C=E\big[\mathrm{log} P(C=c |X_{real})\big]+ E\big[\mathrm{log} P(C=c|X_{fake})\big]
\end{equation}
Dabei wird der Diskriminator wieder so trainiert, dass er den Samples die richtige Quelle zuordnen kann indem versucht wird $L_S$ zu maximieren. Zusätzlich soll noch die Klasse korrekt ermittelt werden, sodass $D$ trainiert wird um $L_S+L_C$ zu maximieren. Im Gegensatz dazu soll der Generator trainiert werden um das entgegengesetzte Ereignis $-L_S$ zu maximieren. Die Klassenlabels sollen dabei trotzdem richtig zugeordnet werden. Somit soll $G$ während des Training $L_C-L_S$ maximieren.\\
\cite{2017arXiv170100160G}, \cite{2016arXiv161009585O}

%
%
\subsection{Attack-Defence}
%
- Das Trainieren eines Klassifikators mit neuronalen Netzen führt trotz der Vielzahl an Parametern in einem Netz zwangsläufig zum einem Overfitting der Trainingsdaten. Dadurch unterscheidet sich der Trainingsfehler immer vom Testfehler. Oftmals werden Regularizierungsterme der Kostenfunktion hinzugefügt um ein Overfitting zu verhindern.\\
- Weitere Methoden bestehen darin den Datensatz durch hinzufügen von zufälligem Rauschen auf die Eingangsdaten zu erweitern oder die Eingangsdaten durch leichtes Rotieren oder Verschieben abzuändern.\\
- insbesondere Gefährdet

Das Trainieren eines neuronalen Netzen als Klassifikators führt trotz der Vielzahl an Parametern in einem Netz zwangsläufig zum einem Overfitting der Trainingsdaten. Oftmals werden Regularizierungsterme der Kostenfunktion hinzugefügt um ein Overfitting zu verhindern. Andere Methoden bestehen darin den Datensatz zu erweitern indem den Eingangsdaten ein zufälliges Rauschen hinzugefügt wird oder die Daten durch Rotieren und Verschieben abgeändert werden.  
-    \\
- Das Trainieren eines neuronalen Netzen als Klassifikators führt trotz der Vielzahl an Parametern in einem Netz zwangsläufig zum einem Overfitting der Trainingsdaten. Oftmals werden Regularizierungsterme der Kostenfunktion hinzugefügt um ein Overfitting zu verhindern. Andere Methoden bestehen darin den Datensatz zu erweitern, indem den Eingangsdaten ein zufälliges Rauschen hinzugefügt wird oder die Daten durch Rotieren und Verschieben abgeändert werden\\
%
- Diese simplen Methoden der Datenerweiterung erreichen oftmals eine gewisse Generalisierung. Allerdings bleiben die Netze dabei anfällig für Störungen in bestimmte Richtungen. Diese Richtung wird adversarial direction genannt und bedeutet eine Richtung in im Inputspace in welche die Klassenwahrscheinlichkeit des Models am meisten sensitiv ist.\\
%
- Diese simplen Methoden der Datenerweiterung erreichen oftmals eine gewisse Generalisierung. Allerdings bleiben die Netze dabei meistens anfällig für gezielte Störungen (adversarial Störungen). Mit dieser Art von Störung sind adversarial Samples gemeint, welche gezielt Abgeändert werden um den Klassifikator zu täuschen. Für ein ursprüngliches (sauberes) Sample $C$ welches vom Klassifikator $M(C)=y_{true}$ richtig klassifiziert wird kann ein adversarial Sample $A$ erstellt werden, sodass der Klassifikator $M(A)\neq y_{true}$ getäuscht wird. Die gezielte Störung kann dabei so gering sein, dass sie für das menschliche Auge nicht ersichtlich ist, aber ausreicht um das neuronale Netz zu einer falschen Klassifikation zu führen. Das Einschleusen solcher adversarial Bilder wird auch adversarial Attacke genannt.\\
%
- Ein Beispiel hierfür ist das in Abbildung X dargestellte Bild eine Pandas. Das saubere Bild (links) wird mit 98\% Sicherheit als Panda klassifiziert. In mittleren Teil wird die Störung gezeigt welche dem Pandabild hinzugefügt wird. Auf der rechten Seite ist das gestört (adversarial) Bild des Pandas dargestellt. Dabei fällt auf, dass für das menschliche Auge kaum eine Störung zu erkennen ist, der Klassifikator das Bild allerdings mit 78\% Sicherheit als Gibbon klassifiziert und nicht mehr als Panda.\\
%
- Durch das Einfügen von adverserial Samples in das Trainingsdatenset kann die Robustheit des Netzes gegen aversarial Samples gesteigert werden. Diese Konzept wird adversarial Training genannt. Darüber hinaus kann das adv. Training das Netz zu generalisieren.\\
%
Im folgenden werden verschiedene Attacke-Methoden gezeigt womit adv. Samples erzeugt werden können. Danach werden die Grundlagen des adv. Trainings präsentiert. Zuletzt wird eine Erweiterung dieser Methode eingeführt, das virtual adversarial Training (VAT). Das VAT ist neben dem VAE und dem GAN eine weitere Methode für semi-überwachtes Lernen.\\
%
\textbf{adv. Attacken}\\
- Die Grundidee ist eine adversarial Richtung zu finden. Damit ist die Richtung im Input space gemeint, auf welche die Klassenwahrscheinlichkeit des Models am meisten sensitiv reagiert.\\
- Eine simple Methode mit geringem Rechenaufwand für das generieren von adv. Samples ist die \textbf{fast gradient sign method} (FGSM). Diese ist definiert als:
\begin{equation}
X^{adv} = X + \epsilon~\mathrm{sign}(\nabla_X J(X,y_{true})),
\end{equation}      
wobei $\epsilon$ die Magnitude der Störung ist. Die FGSM erkundet die Gradientenrichtung der Kostenfunktion und berechnet eine adv. Störung, welche den Wert der Kostenfunktion erhöht. Dazu führt die FGSM einen einzelnen Gradientenupdate durch und geht einen Schritt in Richtung des sign des Gradienten für jedes Pixel.\\
- Eine weitere Methode mit nur einem Gradientenupdate ist die \textbf{one-step target class method}. Ziel dieses Ansatzes ist es die Wahrscheinlichkeit $p(y_{target}|X)$ einer Zielklasse $ y_{target}$ zu bestimmen, welche mit geringer Wahrscheinlichkeit der tatsächlichen Klasse $y_{true}$ für ein Sample $X$ entspricht. Die Formulierung dieser Attacke ist sehr ähnlich zu FGSM nur dass der Gradient in Richtung einer bestimmten Klasse bestimmt wird:
\begin{equation}
X^{adv} = X + \epsilon~\mathrm{sign}(\nabla_X J(X,y_{target})).
\end{equation}
Die Zielklasse kann dabei entweder zufällig gewählt werden oder durch einen "least likely estimator" $y_{LL}=\mathrm{arg~min}_y\{p(y|X)\}$ bestimmt werden.\\
- DIe zwei bisherigen Methoden
%
%
\\Adversarial Machine Learning at scale\\
%
\\VAT paper for semi sup. learning\\ 
%
\\Explaining and harnessing adversarial examples (Bild von panda)\\  
%
- Intro\\
- \textbf{Adversarial Training:}\\
- Attacke Methoden\\
- \textbf{VAT:}\\
%
%
%
\section{Akustic-Preprocessing}
Dieses Kapitel beschreibt die 
